{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_SuicideIntentIIITD_AmitAgarwal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWZ1eiXJ8p0LvobWxmBpOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitagarwal2208/Suicide-Intent-Classification/blob/main/LSTM_SuicideIntentIIITD_AmitAgarwal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X03Il4DijITp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctqa4SzOkLEd"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/AmanuelF/Suicide-Risk-Assessment-using-Reddit/master/anonymized_dataset/500_Reddit_users_posts_labels.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUB7JNZNkN06"
      },
      "source": [
        "df = df.dropna()\n",
        "X=df.drop('Label',axis=1)\n",
        "Y=df['Label']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi34HrSgkZ6O",
        "outputId": "4ea78d34-154e-4f52-a15d-1722676f99ef"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s8oeQPjkcKk"
      },
      "source": [
        "messages = X.copy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RFb5M5zkn-W"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNlk1HPWkqGG"
      },
      "source": [
        "messages = []\n",
        "exclude = ['[',']',',',', ']\n",
        "\n",
        "for i in range(500):\n",
        "  l = df['Post'][i].split(\"'\") \n",
        "  l2 = []\n",
        "  for s in l:\n",
        "    if s not in exclude:\n",
        "      l2.append(s)\n",
        "\n",
        "  messages.append(l2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_XC80kUkseN"
      },
      "source": [
        "### Dataset Preprocessing\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "\n",
        "for i in range(500):\n",
        "  l = []\n",
        "  for text in messages[i]:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    l.append(review)\n",
        "\n",
        "  corpus.append(l) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QLVqWmKkxG0",
        "outputId": "c694153a-9ca7-4773-e1bc-6f5ab5dbab56"
      },
      "source": [
        "corpus[19]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['live other say',\n",
              " 'trigger well cant drink alcohol know much fun want drown drink littl drank today ive issu drink cultur realiti your outsid dont',\n",
              " 'whole life semi stage depress mix period good time moment want neck commit life prevent continu say ill get past point wont matter fuckhead mate wed famili member import event defer fuck world',\n",
              " 'reason peopl like differ shit good way',\n",
              " 'one truli beat depress ive gone fair without major depress stint still get massiv blue day',\n",
              " 'one day hope get ny hold hug',\n",
              " 'work peopl doesnt work other ive alway play sport exercis least three time asthenia biggest low session',\n",
              " 'dog real though',\n",
              " 'yeah theyr best',\n",
              " 'ive never anyon',\n",
              " 'http www southparkstudio com clip beauti sad',\n",
              " 'went got realli drunk told peopl work day later told go see doctor put lexapro got serious high day',\n",
              " 'got total fuck one night sex girl fuck amaz almost miss flight home wasnt sure fuck happend got number proceed text proof anyon chick exist besid fuck mind sex came visit ks away hadnt got plane would type',\n",
              " 'got first last month mayb got second last mayb bird accept dont know honestli dont kill tri think mayb anoth bird ive figur chick reach late theyr look build nest find women age rang late earli singl need build nest',\n",
              " 'woodi allen movi actual reason live',\n",
              " 'hard say realli debt slight issu loss close friend lack socialis stand still job',\n",
              " 'disagre agre distroid advic medic fuck help consult learn person isnt work dont take bowl shit take respons life dont make excus control certain aspect life thing cannot control worri depress feel shit isnt bad feel shit understand question listen shit self help tape shit give someth think also help ask question dont give',\n",
              " 'save',\n",
              " 'mg worst shit ever felt like drug addict come guess took three month get withdraw symptom go away first start use went away weekend forgot med almost didnt get let plane home',\n",
              " 'friend never get one best mate live time kill oversea housem also great friend found hang room dont get',\n",
              " 'isol crowd surround peopl feel like rat stink give feel whatev cant fuck escap tri claw surfac your stuck your stuck stench life miseri roach swarm breath stale fuck air use parasit never abl shed',\n",
              " 'motion pictur sound track kill',\n",
              " 'mother die motor neuron diseas shit real couldnt speak would write stuff like dont love fuck kill dont know relat fuck suck whole famili fuck seriou amount time watch mum die coupl year gave seriou mare',\n",
              " 'thought id found someon reject becam awar depress sympathi sex okay',\n",
              " 'boat bud still degre contact girl drunken intercours late start chang girl want settl late theyll settl guy like us real easi show em care',\n",
              " 'call livejourn',\n",
              " 'much better text morn drunk',\n",
              " 'nice patronis',\n",
              " 'offer mayb one thing everi day dont want stuff procrastin stuff think worth get someth',\n",
              " 'youv got visualis',\n",
              " 'went seriou bad trot told mate didnt realli give fuck year later anoth mate kill oversea time wake afterward like take warn know went pete look ive hardli heard seen guy last three year everyon disappear',\n",
              " 'peopl roll wouldnt know half shit like guess hard know ive found though key peopl question give fuck',\n",
              " 'see way depress pretti hard gener step take work dont would tri one thing day didnt want regardless small got slowli better frame mind life stop chore day rare dark day though still happen never like last day im awar happen dont let get suck',\n",
              " 'thought decent film fuck decent',\n",
              " 'someth forget keep tri live tri accept without still far go thank remind',\n",
              " 'get drunk realli want neck rare drink time want neck',\n",
              " 'id suggest pill great experi wake almost job quit could wors situat leav someon els make decis',\n",
              " 'tough gig feel bit aa say havent rough day sinc last year seem almost long rememb without feel im realli sure chang financi situat isnt better circl friend hasnt fatigu decreas dont relationship real thing happen dont drink much well stuff im commit fit turn around use go run exercis would experi worst low ever afterward dont feel great feel good honestli dont know thing chang outsid noth realli say keep thing like self help tape done anyth one steve chandler realli one point someth everyday like stuff procrastin motiv short period',\n",
              " 'need explos gun fire',\n",
              " 'that pretti depress actual',\n",
              " 'yeah scale ice',\n",
              " 'im crash dude talk later',\n",
              " 'gt happen tomorrow happen yesterday day week month year mean abso fuck lute noth live moment eh exist futil',\n",
              " 'mayb cloud silver line',\n",
              " 'got told play video game dont see fuck help',\n",
              " 'thing went like late work relat back injuri anti depress found mother termin ill got small payout move back home close famili start busi bro sister didnt pay use settl busi support stop take med move away mother die weekend quit next job move good friend seriou credit card damag friend kill close group friend fall apart coupl month later k hole work ever sinc get free im still k',\n",
              " 'read catcher rye hundr time didnt work im still aliv right',\n",
              " 'prefer call radelaid',\n",
              " 'context',\n",
              " 'ive never sustain relationship dont think ive ever sex havent drunk come big night depress fuck suck like butter say id rather experi loss never experi gain shit quot better love lost never love',\n",
              " 'theyll bake cake stage banana cake exact',\n",
              " 'woodi allen movi watch list dont know kill would terribl idea need tip screen write help want make short film holden caulfield play game checker jane gallagh think approach',\n",
              " 'opinion would dont take medic involv ssri snri like efexor shit fuck chao get',\n",
              " 'hardest part know want die beauti girl top today last night well depress hell told didnt want night came around today still felt like neck dont know fuck would want neck someon say ador',\n",
              " 'realli like bird seen month realli bad asthenia edg saw took sympathi sex shut next day drop hospit im still good time guess',\n",
              " 'pit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbjMR4JnlOFU"
      },
      "source": [
        "corpusConcat = []\n",
        "for i in range(500):\n",
        "  s = \"\" \n",
        "  for t in corpus[i]:\n",
        "    s = s+t \n",
        "  corpusConcat.append(s)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk1xdr6UnUSO"
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYDkhfaTnpTj"
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCx1n9SKntrR"
      },
      "source": [
        "def get_max_length(corpusConcat):\n",
        "    \n",
        "    max_length = 0\n",
        "    for sentence in corpusConcat:\n",
        "      max_length = max(max_length,len(sentence))\n",
        "    return max_length\n",
        "\n",
        "def get_word2vec_enc(corpus):\n",
        "    encoded_text = []\n",
        "    for text in corpus:\n",
        "        tokens = text.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_text.append(word2vec_embedding)\n",
        "    return encoded_text\n",
        "\n",
        "\n",
        "def get_padded_encoded_text(encoded_text):\n",
        "\n",
        "    max_length = get_max_length(encoded_text)\n",
        "    padded_text_encoding = []\n",
        "    for enc_text in encoded_text:\n",
        "        zero_padding_cnt = max_length - enc_text.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_text = np.concatenate((pad, enc_text), axis=0)\n",
        "        padded_text_encoding.append(enc_text)\n",
        "    return padded_text_encoding\n",
        "\n",
        "\n",
        "\n",
        "def label_encode(label):\n",
        "  \n",
        "    if label == 'Attempt':\n",
        "        return [1,0,0,0,0]\n",
        "    elif label == 'Behavior':\n",
        "        return [0,1,0,0,0]\n",
        "    elif label == 'Ideation':\n",
        "        return [0,0,1,0,0]\n",
        "    elif label == 'Indicator':\n",
        "        return [0,0,0,1,0]\n",
        "    elif label == 'Supportive':\n",
        "        return [0,0,0,0,1]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN7jN_Efov38"
      },
      "source": [
        "encoded_text = get_word2vec_enc(corpusConcat)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC1fotYupDhI"
      },
      "source": [
        "padded_encoded_text = get_padded_encoded_text(encoded_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34T7OtmwpJeI"
      },
      "source": [
        "encoded_labels = [label_encode(label) for label in Y]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcuoFxvOrfII"
      },
      "source": [
        "X1 = padded_encoded_text\n",
        "X1 = np.array(X1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwjFyFGxrxaW"
      },
      "source": [
        "Y1 = encoded_labels\n",
        "Y1 = np.array(Y1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbT4wKlIsFdz"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWvTxmrd0FhJ"
      },
      "source": [
        "X_train = X1[:375]\n",
        "X_test = X1[375:]\n",
        "Y_train = Y1[:375]\n",
        "Y_test = Y1[375:]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_DnLrtd0QG-",
        "outputId": "fbaea1c2-834b-48bf-b209-c586fd0a3084"
      },
      "source": [
        "Y1.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gi8Xg7cr71w",
        "outputId": "c143aa05-5afe-4ff1-982e-323b86435f75"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "          epochs=4, batch_size=12)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "32/32 [==============================] - 90s 3s/step - loss: 1.5599 - accuracy: 0.3120 - val_loss: 1.5349 - val_accuracy: 0.3200\n",
            "Epoch 2/4\n",
            "32/32 [==============================] - 87s 3s/step - loss: 1.5288 - accuracy: 0.3413 - val_loss: 1.5292 - val_accuracy: 0.3200\n",
            "Epoch 3/4\n",
            "32/32 [==============================] - 87s 3s/step - loss: 1.5208 - accuracy: 0.3493 - val_loss: 1.5281 - val_accuracy: 0.3200\n",
            "Epoch 4/4\n",
            "32/32 [==============================] - 87s 3s/step - loss: 1.5069 - accuracy: 0.3493 - val_loss: 1.5324 - val_accuracy: 0.3120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8740d5d090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3gKGCpFr_rR"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}